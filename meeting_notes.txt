November 5 - Meeting with Tiwa

Current LMs are open vocabulary - so they can express any word. So we need to specify
the content of our language model's vocab - otherwise, calls it <UNK>. Humans solve the problem
of segmenting words from speech.

Most LMs start by looking at a doc full of characters and iteratively merge into larger and
larger chunks that looks like words, then stop.

This project is taking a closer look at the way LMs are solving the problem and characterising
it in ways that we think humans solve this problem.

How to improve upon word segmentation?
- look at
- byte-pair encoding: iteratively merging the most frequent characters into larger and larger
chunks
- word-piece encoding: iterative merging based on the effect they have on LM estimates in the corpus - morpheme-level PCFG (good because it takes into account tree structure and captures the language as inference thing), neurosymbolic models, etc.
- take a tiny NLM and distil knowledge from RNNG or PCFG, word encoding, compare resultant schemes
- proposing an encoding scheme is significant work

Plan:
1. Implement and improve Norvig's model
2. Look into 9.19 notes and n-gram models there
3. test on new data?? - make human data
4. other metrics than probability - surprisal? human reading time?

Meeting 12/12
describe PCFG + char level PCFG/n-gram/small NN
    - captures word segmentation as inference
    - we made one, analysed head-to-head with a statistical model
word level RNN
word segmentation as inference - Goldwater (structure algorithm)
how is this model different from word piece
how do babies do it from speech without existing vocab (like this model)
PCFG word level inference
compositionality, structure
probmods - PCFG, HMMs
word learning class - compare to Pw, ngram model
